# Interesting-DL-Papers
List of DL papers on Interpretability, Learning from Limited Data and Information Bottleneck

## Visual Interpretation & Adversarial Attacks
[1] [Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer](https://arxiv.org/pdf/1612.03928.pdf)

[2] [Interpretable and Pedagogical Examples](https://arxiv.org/pdf/1711.00694.pdf)

[3] [Interpretable Explanations of Black Boxes by Meaningful Perturbation](https://arxiv.org/pdf/1704.03296.pdf)

[4] [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)

[5] [Grad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks](https://arxiv.org/pdf/1610.02391.pdf)

[6] [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/pdf/1802.00420.pdf)

## Information Bottleneck
[1] [Compressing Neural Networks using the Variational Information Bottleneck](https://arxiv.org/pdf/1802.10399.pdf)

[2] [One Big Net For Everything](https://arxiv.org/pdf/1802.08864.pdf)

## Model Level Changes
[1] [Minimal gated unit for recurrent neural networks](https://link.springer.com/article/10.1007/s11633-016-1006-2)

[2] [Gated Feedback Recurrent Neural Networks](http://proceedings.mlr.press/v37/chung15.pdf)

[3] [MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks](https://arxiv.org/abs/1711.06788)

[4] [A recurrent neural network without chaos](https://arxiv.org/abs/1612.06212)
## Limited Data


